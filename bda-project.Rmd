---
title: "project"
author: "anon"
date: "11/22/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(aaltobda)
library(rstan)
library(loo)
library(latex2exp)
library(bayesplot)
rstan_options (javascript = FALSE)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# Modeling the data 

In the following, we describe the model used for our experiments. 
We posses a dataset of papers from two EMNLP 2020 venues: Main conference and findings,
which are indicated when necessary with indices $m$ and $f$, respectively (where possible, we omit these for readability). 
For each venue, we have $M$ and $N$ total observations, with two features $y_i^{(1)}$ and $y_i^{(2)}$ each.

The first one denotes the number of citations, which we model using a Poisson distribution and a Gamma prior:

$$ 
  y_i^{(1)} \sim \text{Poisson}(\lambda)\\    
  \lambda \sim \Gamma(\alpha_\lambda, \beta_\lambda)
$$

The second one described whether paper is a short or a long paper, which we model using a Bernoulli with a Beta prior:

$$
  y_i^{(2)} \sim \text{Bernoulli}(\rho) \\
  \rho \sim \text{B}(\alpha_\rho, \beta_\rho)
$$
Because we suspect that longer papers receive more citations on average (since they have more content, often receive posters and spotlight talks etc.), we now modify the likelihood for the first feature in the following way:

$$ 
  y_i^{(1)} \sim \text{Poisson}(\lambda\exp(y_i^{(2)}))\\
$$

Thus, being a long paper makes a higher number of citations become more likely.
This leads us to the following likelihood and posterior for a single sample:

$$
  p(y_i|\lambda, \rho) = p(y_i^{(1)}|\lambda, y_i^{(2)})p(y_i^{(2)}|\rho)\\
$$

Further, we can formulate the following posterior for the entire Dataset $\mathbb{D} = \{y_1, \ldots, y_N\}$:
$$
  p(\lambda|\rho, \mathbb{D}, \theta) \propto \prod_{i=1}^N p(y_i^{(1)}|\lambda, y_i^{(2)})p(\lambda|\alpha_\lambda, \beta_\lambda)p(y_i^{(2)}|\rho)p(\rho|\alpha_\rho, \beta_\rho)
$$

where we summarize all prior parameters as $\theta = [\alpha_\lambda, \beta_\lambda, \alpha_\rho, \beta_\rho]$ to avoid clutter. As we see in the the next section, we would actually like to reason about the posterior $p(\lambda, |\mathbb{D}, \theta)$. 

[//]: # "Since we chose conjugate prior for the two feature likelihoods and assumed the features to be independent, we know that the posterior $p(\lambda, \rho|\mathbb{D})$ is a product of a Beta with posterior parameters $\alpha_\rho + \sum_{i=1}^N y^{(2)}_i, \beta_\rho + N - \sum_{i=1}^N y^{(2)}_i$ and a Gamma with posterior parameters $\alpha_\lambda + \sum_{i=1}^N y^{(1)}_i, \beta_\lambda + N$ (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Conjugate_prior) for this standard result)."

# Research question

We are interested in finding out whether publishing at the main conference or finding has an impact on one's citations. 
For this purpose, we would like to compare the posterior distributions $p(\lambda_m|\rho_m, \mathbb{D}_m, \theta_m) \stackrel{?}{=} p(\lambda_f|\rho_f, \mathbb{D}_f, \theta_f)$. 
Since $\lambda_m, \lambda_f$ can in this context be interpreted as the mean citation rates for both venues, comparing their respective posterior distributions allows us to answer this question given the data and taking uncertainty into account.

# Using a hierarchical model

We can choose to model the distributions of interest - $p(\lambda_m|\rho_m, \mathbb{D}_m, \theta_m)$ and $p(\lambda_f|\rho_f, \mathbb{D}_f, \theta_f)$ - each with their separate priors. However, we also consider a shared hyperprior for the respective $\lambda$ and $\rho$ parameters. Intuitively, this implies that we either assume our prior belief about the citation rate of papers at both venues to be the same OR the range of probabilities to be a long or short paper at both venues to be similar. The generative stories of such hierarchical models is given below. 

Shared hyperprior for $\lambda$:
$$
  \lambda_m, \lambda_f \sim \Gamma(\alpha_0, \beta_0)\\
  \rho_m \sim \text{B}(\alpha_{\rho_m}, \beta_{\rho_m})\\
  \rho_f \sim \text{B}(\alpha_{\rho_f}, \beta_{\rho_f})\\
$$

Shared hyperprior for $\rho$:

$$
  \lambda_m \sim \Gamma(\alpha_m, \beta_m)\\
  \lambda_f \sim \Gamma(\alpha_f, \beta_f)\\
  \rho_m, \rho_f \sim \text{B}(\alpha_0, \beta_0)\\
$$


# R and Stan Code


```{r}
list.files()
```

```{r}
findings_df <- read.csv("findings.tsv_type.csv")
findings_df$journal = "findings"
head(findings_df)

main_df <- read.csv("main.tsv_type.csv")
main_df$journal = "main"
head(main_df)
```

```{r}
dim(main_df)
dim(findings_df)
data <- rbind(main_df, findings_df)

ggplot(data, aes(Citation, color=journal, fill=journal)) +
  geom_density(alpha=0.2) +
  coord_cartesian(xlim=c(0, 50))
```

# Analyze the distribution of citations across long / short papers

```{r}
long_papers = subset(data, type == "long")[, "Citation"]
short_papers = subset(data, type == "short")[, "Citation"]

median(short_papers)
mean(short_papers)
median(long_papers)
mean(long_papers)

#sampled_long_papers = sample(long_papers, size=length(short_papers))
hist_long = density(long_papers)
hist_short = density(short_papers)
plot(hist_long, col=rgb(0,0,1,1/4), xlim=c(0,30), ylim=c(0, 0.12), main="Citations by paper length", xlab="Number of citations")  # first histogram
polygon(hist_long, col=rgb(0,0,1,1/4), border=rgb(0,0,1,1/4)) 
lines(hist_short, col=rgb(1,0,0,1/4), xlim=c(0,30))  # second
polygon(hist_short, col=rgb(1,0,0,1/4), border=rgb(1,0,0,1/4)) 
legend("topright", legend=c("Long Papers", "Short Papers"), col=c(rgb(0,0,1,1/4), rgb(1,0,0,1/4)), lty=1:1, cex=0.8)
```
## Closed form solutions to the basic model

$$
  y_i \sim Poisson(\lambda) \\
  \lambda \sim \Gamma(\alpha, \beta)\\

  \lambda | Y \sim \Gamma(\alpha + \sum^n_{i=0}y_i, \beta + n)
$$

```{r}


sample_closed_draws <- function(samples, n_draws, alpha = 0.001, beta = 0.001) {
  n <- length(samples)
  closed_draws <- rgamma(n_draws, alpha + sum(samples), beta + n)  
  return(data.frame(lambda=closed_draws))
}
close_main_draws <- sample_closed_draws(main_df$Citation, 4000)
close_main_draws$venue <- "main"
close_find_draws <- sample_closed_draws(findings_df$Citation, 4000)
close_find_draws$venue <- "findings"

plotting_data <- rbind(close_main_draws, close_find_draws)
ggplot(plotting_data, aes(lambda, color=venue, fill=venue)) +
  geom_histogram(bins=100, alpha=.5) +
  theme(legend.position="top") +
  geom_vline(aes(xintercept=mean(close_main_draws$lambda)), linetype="dashed", size=.5) + 
  geom_vline(aes(xintercept=mean(close_find_draws$lambda)), linetype="dashed", size=.5)
```

investigate diff and the "probability" of there being overlap between the predictive posterior distribution
```{r}
main_prediction_posterior <- sapply(close_main_draws$lambda, function(lambda) rpois(1, lambda))
findings_prediction_posterior <- sapply(close_find_draws$lambda, function(lambda) rpois(1, lambda))
transformed_dist <- main_prediction_posterior - findings_prediction_posterior
predicted_mean <- mean(transformed_dist)
```

```{r}
a <- data.frame(lambda=main_prediction_posterior)
a$venue = "main"
b <- data.frame(lambda=findings_prediction_posterior)
b$venue = "findings"

ggplot(rbind(a,b), aes(lambda, color=venue, fill=venue)) +
  geom_density(alpha=.5) +
  labs(title="predictive posterior draws", x=TeX("\\tilde{y}"), y="")
```
```{r}
data <- rbind(main_df, findings_df)

truncate <- function(data_df, p, name, n_samples = 4000) {
  mask <- data_df$Citation < quantile(data_df$Citation, p)
  truncated_data_df <- data_df[mask, ]
  
  lambda_draws <- sample_closed_draws(truncated_data_df$Citation, n_samples)
  predictive_draws <- sapply(lambda_draws$lambda, function(lambda) rpois(1, lambda))
  ys <- data.frame(Citation=predictive_draws)
  ys$Journal <- name
  
  return(ys)
}

threshold <- .75
tmp_main_df <- truncate(main_df, threshold, "predictive_main(truncated)")
tmp_find_df <- truncate(findings_df, threshold, "predictive_findings(truncated)")
tmp_df <- rbind(tmp_main_df, tmp_find_df)

tmp = data.frame(Citation=data$Citation, Journal=data$journal)
tmp = rbind(tmp, tmp_df)

ggplot(tmp, aes(Citation, color=Journal, fill=Journal)) +
  geom_density(alpha=0.2) +
  coord_cartesian(xlim=c(0, 50))


```




```{r}
ggplot(data.frame(y_hat = transformed_dist), aes(y_hat)) +
  geom_histogram(bins=35, color="black", fill="lightblue", alpha=.6) + 
  geom_vline(aes(xintercept=mean(predicted_mean)), linetype="dashed", size=.5) +
  labs(title="", x=TeX("\\tilde{y}_m - \\tilde{y}_f"), y="") +
  annotate("text", x=10, y=250, label= paste("mean =", round(predicted_mean, 2)))
P <- ecdf(transformed_dist)
eps <- 1
P(0+eps)-P(0-eps)
```

$$
P(|y_m - y_f| \leq 1) = 0.16
$$

# Creating the model

```{r}
# processed data
sizes <- c(nrow(main_df), nrow(findings_df))
data <- rbind(main_df, findings_df)$Citation
sizes
# data
```

```{r}
get_weakly_informative_gamma = function(samples) {
  alpha_est = mean(samples)^2 / (max(samples) - min(samples));
  beta_est = alpha_est / mean(samples);
  
  return(c(alpha_est, beta_est));
}

get_weakly_informative_normal = function(samples_short, samples_long) {
  eps = 1e-12
  lower = min(min(samples_short + eps) / max(samples_long), min(samples_long + eps) / max(samples_short))
  upper = max(samples_long) / mean(samples_short)
  mu = mean(samples_long) / mean(samples_short)
  sigma = (upper - mu) / 2.576
  
  return(c(mu, sigma));
}



get_weakly_informative_gamma(main_df$Citation)
```



```{r}
model_data <- list(N = length(data),
                   sizes = sizes,
                   J = 2,
                   y = data)

fit <- stan(file = "model_mixture.stan", 
            data = model_data,
            chains = 4,
            iter = 2000,
            warmup = 1000)
```



```{r}
draws <- as.data.frame(fit)
apply(array, margin, ...)
```


## posterior predictive draws

```{r}
draws <- as.data.frame(fit)
yrep <- extract(fit, permuted=FALSE)
dimnames(yrep)

y_main <- main_df$Citation
y_findings <- findings_df$Citation
yrep_main <- matrix(yrep[,,3], nrow=4, ncol=length(y_main))
yrep_findings <- matrix(yrep[,,4], nrow=4, ncol=length(y_findings))

print(length(y_main))
color_scheme_set("viridis")
ppc_dens_overlay(y_main, yrep_main) + ggtitle('Main') + xlim(0,100) + yaxis_text() + theme(text = element_text(size = 16, family="sans"))     
ppc_dens_overlay(y_findings, yrep_findings) + ggtitle('Findings') + xlim(0,100)  + yaxis_text() + theme(text = element_text(size = 16, family="sans"))   
```

